services:
  workspace:
    image: mcr.microsoft.com/devcontainers/typescript-node:22
    volumes:
      - .:/workspace
      - /var/run/docker.sock:/var/run/docker.sock
    working_dir: /workspace
    command: sleep infinity

  frontend:
    build:
      context: .
      dockerfile: ./frontend/Dockerfile
    ports:
      - "3000:3000"
    environment:
      - NODE_ENV=development
      - NEXT_PUBLIC_API_URL=http://localhost:8000
      - NEXT_PUBLIC_BACKEND_URL=http://localhost:8000
      - NEXT_PUBLIC_WS_URL=ws://localhost:8000
      - NEXT_PUBLIC_VSCODE_URL=http://localhost:8080
      - NEXT_PUBLIC_MCP_WS_URL=ws://localhost:9002
      - BACKEND_URL=http://backend:8000
      - NODE_OPTIONS=--dns-result-order=ipv4first
      - HOSTNAME=0.0.0.0
      # Allow Node.js to resolve hoisted packages from root node_modules
      - NODE_PATH=/workspace/node_modules
    volumes:
      - ./frontend:/app
      - ./shared:/shared
      - /app/node_modules
      - /app/.next
      # Mount root node_modules as read-only for hoisted package resolution
      - ./node_modules:/workspace/node_modules:ro
    depends_on:
      - backend
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 1G
        reservations:
          cpus: '0.5'
          memory: 512M
    logging:
      driver: "json-file"
      options:
        max-size: "5m"
        max-file: "2"

  backend:
    build:
      context: .
      dockerfile: ./backend/Dockerfile
    ports:
      - "8000:8000"
    extra_hosts:
      - "host.docker.internal:host-gateway"
    environment:
      - NODE_ENV=development
      - LOG_LEVEL=${LOG_LEVEL:-warn}
      - PORT=8000
      - DB_HOST=postgres
      - DB_PORT=5432
      - DB_USERNAME=postgres
      - DB_PASSWORD=password
      - DB_NAME=coding_agent
      - DB_SYNCHRONIZE=true
      - LLM_SERVICE_URL=http://llm-service:9000
      - VECTOR_DB_URL=http://qdrant:6333
      - OLLAMA_HOST=http://ollama:11434
      # MCP server is in vscode network namespace, access via vscode service
      - MCP_SERVER_URL=http://vscode:9001
      # API keys explicitly unset to avoid validation errors
      - OPENAI_API_KEY=
      - ANTHROPIC_API_KEY=
      - COHERE_API_KEY=
      - REDIS_URL=redis://redis:6379
      - FRONTEND_URL=http://localhost:3000
      - JWT_SECRET=${JWT_SECRET:-your_jwt_secret_key_here_make_it_long_and_secure_for_production_use_at_least_32_chars}
      - SESSION_SECRET=${SESSION_SECRET:-your_session_secret_key_here_make_it_long_and_secure_for_production_use_at_least_32_chars}
      # Allow Node.js to resolve hoisted packages from root node_modules
      - NODE_PATH=/workspace/node_modules
    volumes:
      - ./backend:/app
      - ./shared:/shared
      - backend_node_modules:/app/node_modules
      - ./workspace:/workspace:cached
      # Mount root node_modules as read-only for hoisted package resolution
      - ./node_modules:/workspace/node_modules:ro
    command: ["npm", "run", "start:dev"]
    depends_on:
      - qdrant
      - postgres
      - redis
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 2G
        reservations:
          cpus: '1.0'
          memory: 1G
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  llm-service:
    build:
      context: .
      dockerfile: ./llm-service/Dockerfile
    ports:
      - "9000:9000"
    extra_hosts:
      - "host.docker.internal:host-gateway"
    environment:
      - NODE_ENV=production
      - LOG_LEVEL=${LOG_LEVEL:-warn}
      - BACKEND_URL=http://backend:8000
      - OLLAMA_HOST=http://ollama:11434
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY}
      - HUGGINGFACE_API_KEY=${HUGGINGFACE_API_KEY:-}
      - MODEL_CACHE_DIR=/models/cache
      - REDIS_URL=redis://redis:6379
      # Allow Node.js to resolve hoisted packages from root node_modules
      - NODE_PATH=/workspace/node_modules
    volumes:
      - ./llm-service:/app
      - ./shared:/shared
      - ./models:/models
      - /app/node_modules
      # Mount root node_modules as read-only for hoisted package resolution
      - ./node_modules:/workspace/node_modules:ro
    depends_on:
      - redis
    deploy:
      resources:
        limits:
          cpus: '4.0'
          memory: 4G
        reservations:
          cpus: '2.0'
          memory: 2G
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  qdrant:
    image: qdrant/qdrant:latest
    platform: linux/amd64
    ports:
      - "6333:6333"
    volumes:
      - ./qdrant_data:/qdrant_data
    environment:
      - QDRANT__SERVICE__HTTP_PORT=6333
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 2G
        reservations:
          cpus: '1.0'
          memory: 1G
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "2"

  ollama:
    image: ollama/ollama:latest
    platform: linux/amd64
    restart: unless-stopped
    ports:
      - "11434:11434"
    volumes:
      - ./ollama_data:/root/.ollama
      - ./scripts/ollama-preload.sh:/usr/local/bin/ollama-preload.sh:ro
    environment:
      - OLLAMA_ORIGINS=*
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_NUM_PARALLEL=2        # Allow 2 parallel requests
      - OLLAMA_MAX_LOADED_MODELS=1   # Keep one model loaded
      - OLLAMA_KEEP_ALIVE=30m        # Keep model in memory longer
      - OLLAMA_NUM_CTX=8192          # Increased context window
      - OLLAMA_NUM_THREAD=6          # Use 6 CPU threads
      - OLLAMA_NUM_GPU=0             # No GPU available
      - PRELOAD_MODELS=llama3.1:latest  # Model to preload on startup
    runtime: runc
    entrypoint: ["/bin/bash", "/usr/local/bin/ollama-preload.sh"]
    deploy:
      resources:
        limits:
          cpus: '6.0'
          memory: 12G
        reservations:
          cpus: '3.0'
          memory: 6G
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "2"

  vscode:
    build:
      context: ./vscode
      dockerfile: Dockerfile
    platform: linux/amd64
    restart: unless-stopped
    # Make IPC namespace shareable for mcp-server
    ipc: shareable
    ports:
      - "8080:8080"
      - "9001:9001"  # MCP server HTTP port (shared network namespace)
      - "9002:9002"  # MCP server WebSocket port (shared network namespace)
    environment:
      - SHELL=/bin/bash
      - DISABLE_WORKSPACE_TRUST=true
      - VSCODE_PROXY=true  # Enable proxy mode for dev servers
    volumes:
      - ./workspace:/home/coder/workspace:cached
      - vscode-data:/home/coder/.local/share/code-server
      - vscode-extensions:/home/coder/.local/share/code-server/extensions
    command: >
      --bind-addr 0.0.0.0:8080
      --auth none
      --disable-telemetry
      --disable-update-check
      --disable-workspace-trust
      --disable-file-downloads
      --disable-getting-started-override
      /home/coder/workspace
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 1G
        reservations:
          cpus: '0.5'
          memory: 512M
    logging:
      driver: "json-file"
      options:
        max-size: "5m"
        max-file: "2"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  # vscode-proxy: Not needed for development - use VSCode directly on port 8080
  # Uncomment for production if you need authentication
  # vscode-proxy:
  #   build:
  #     context: ./vscode-proxy
  #     dockerfile: Dockerfile
  #   ports:
  #     - "8081:8081"
  #   environment:
  #     - NODE_ENV=production
  #     - VSCODE_PROXY_PORT=8081
  #     - VSCODE_URL=http://vscode:8080
  #     - JWT_SECRET=${JWT_SECRET:-your_jwt_secret_key_here_make_it_long_and_secure_for_production_use_at_least_32_chars}
  #     - FRONTEND_URL=${FRONTEND_URL:-http://localhost:3000}
  #     - REQUIRE_AUTH=true
  #   depends_on:
  #     - vscode

  mcp-server:
    build:
      context: .
      dockerfile: ./mcp-server/Dockerfile
    # Share network namespace with vscode for localhost-speed communication
    network_mode: "service:vscode"
    # Share IPC namespace for better inter-process communication
    ipc: "service:vscode"
    environment:
      - NODE_ENV=development
      - MCP_PORT=9001
      - VSCODE_HOST=localhost
      - VSCODE_PORT=8080
      - LLM_SERVICE_URL=http://llm-service:9000
      - BACKEND_URL=http://backend:8000
      - WORKSPACE_PATH=/workspace
      # Allow Node.js to resolve hoisted packages from root node_modules
      - NODE_PATH=/workspace/node_modules
    volumes:
      # Use consistent workspace path and cached mount for better performance
      - ./workspace:/workspace:cached
      # Mount root node_modules as read-only for hoisted package resolution
      - ./node_modules:/workspace/node_modules:ro
    depends_on:
      - vscode
      - backend
      - llm-service
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 1G
        reservations:
          cpus: '0.5'
          memory: 512M
    logging:
      driver: "json-file"
      options:
        max-size: "5m"
        max-file: "2"

  postgres:
    image: postgres:15
    platform: linux/amd64
    ports:
      - "5432:5432"
    environment:
      - POSTGRES_DB=coding_agent
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD=password
    volumes:
      - ./postgres_data:/var/lib/postgresql/data
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 1G
        reservations:
          cpus: '0.5'
          memory: 512M
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "2"

  redis:
    image: redis:7-alpine
    platform: linux/amd64
    ports:
      - "6379:6379"
    volumes:
      - ./redis_data:/data
    command: redis-server --maxmemory 256mb --maxmemory-policy allkeys-lru
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 512M
        reservations:
          cpus: '0.5'
          memory: 256M
    logging:
      driver: "json-file"
      options:
        max-size: "5m"
        max-file: "2"

volumes:
  backend_node_modules:
  vscode-extensions:
  vscode-data:

networks:
  default:
    name: coding-agent-network